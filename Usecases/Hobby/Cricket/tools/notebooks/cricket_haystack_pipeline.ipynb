{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb027d31",
   "metadata": {},
   "source": [
    "# Cricket Data Semantic Search & Conversational Pipeline with Haystack\n",
    "\n",
    "This notebook demonstrates a full Haystack pipeline for cricket player data, including:\n",
    "- Data loading and preprocessing\n",
    "- Vectorization with GPT-4.0 embeddings\n",
    "- Semantic storage in a vector database (FAISS)\n",
    "- Conversational UI for querying the data\n",
    "- Efficient conversation persistence\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Install Required Packages and Databases\n",
    "\n",
    "The following commands will install all necessary dependencies and set up the FAISS vector database in the `Databases` folder. To delete the database, simply remove the corresponding files from the folder.\n",
    "\n",
    "```bash\n",
    "# Install core dependencies\n",
    "pip install farm-haystack[faiss] openai streamlit\n",
    "\n",
    "# (Optional) If you want to use Weaviate or Qdrant instead of FAISS:\n",
    "# pip install farm-haystack[weaviate]\n",
    "# pip install farm-haystack[qdrant]\n",
    "\n",
    "# FAISS is file-based and will store its index in the Databases folder\n",
    "mkdir -p ../../../../Databases/faiss\n",
    "```\n",
    "\n",
    "To delete the FAISS database, run:\n",
    "```bash\n",
    "rm -rf ../../../../Databases/faiss\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80dbc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever, PromptNode, PromptTemplate\n",
    "from haystack.pipelines import Pipeline\n",
    "import openai\n",
    "import streamlit as st\n",
    "import sqlite3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930508a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Load and Preprocess Cricket Data\n",
    "\n",
    "We will load both the stats and personal information CSVs, merge them, and prepare the data for semantic vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c1acba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player_id</th>\n",
       "      <th>name</th>\n",
       "      <th>role</th>\n",
       "      <th>batting_average</th>\n",
       "      <th>bowling_average</th>\n",
       "      <th>team</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>Batsman</td>\n",
       "      <td>57.7</td>\n",
       "      <td>N/A</td>\n",
       "      <td>India</td>\n",
       "      <td>1988-11-05</td>\n",
       "      <td>India</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Name: Virat Kohli. Role: Batsman. Team: India....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Steve Smith</td>\n",
       "      <td>Batsman</td>\n",
       "      <td>59.8</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Australia</td>\n",
       "      <td>1989-06-02</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Name: Steve Smith. Role: Batsman. Team: Austra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Kane Williamson</td>\n",
       "      <td>Batsman</td>\n",
       "      <td>54.3</td>\n",
       "      <td>N/A</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>1990-08-08</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Tauranga</td>\n",
       "      <td>Name: Kane Williamson. Role: Batsman. Team: Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Joe Root</td>\n",
       "      <td>Batsman</td>\n",
       "      <td>50.2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>England</td>\n",
       "      <td>1990-12-30</td>\n",
       "      <td>England</td>\n",
       "      <td>Sheffield</td>\n",
       "      <td>Name: Joe Root. Role: Batsman. Team: England. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>Batsman</td>\n",
       "      <td>48.6</td>\n",
       "      <td>N/A</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>1994-10-15</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Lahore</td>\n",
       "      <td>Name: Babar Azam. Role: Batsman. Team: Pakista...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   player_id             name     role batting_average bowling_average  \\\n",
       "0          1      Virat Kohli  Batsman            57.7             N/A   \n",
       "1          2      Steve Smith  Batsman            59.8             N/A   \n",
       "2          3  Kane Williamson  Batsman            54.3             N/A   \n",
       "3          4         Joe Root  Batsman            50.2             N/A   \n",
       "4          5       Babar Azam  Batsman            48.6             N/A   \n",
       "\n",
       "          team date_of_birth      country       city  \\\n",
       "0        India    1988-11-05        India      Delhi   \n",
       "1    Australia    1989-06-02    Australia     Sydney   \n",
       "2  New Zealand    1990-08-08  New Zealand   Tauranga   \n",
       "3      England    1990-12-30      England  Sheffield   \n",
       "4     Pakistan    1994-10-15     Pakistan     Lahore   \n",
       "\n",
       "                                                text  \n",
       "0  Name: Virat Kohli. Role: Batsman. Team: India....  \n",
       "1  Name: Steve Smith. Role: Batsman. Team: Austra...  \n",
       "2  Name: Kane Williamson. Role: Batsman. Team: Ne...  \n",
       "3  Name: Joe Root. Role: Batsman. Team: England. ...  \n",
       "4  Name: Babar Azam. Role: Batsman. Team: Pakista...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cricket player stats and personal info\n",
    "data_dir = '../../../../Hobby/Cricket/data'\n",
    "stats_path = os.path.join(data_dir, 'cricket_player_stats.csv')\n",
    "personal_path = os.path.join(data_dir, 'cricket_player_personal.csv')\n",
    "\n",
    "stats_df = pd.read_csv(stats_path)\n",
    "personal_df = pd.read_csv(personal_path)\n",
    "\n",
    "# Merge on player_id and fill missing values\n",
    "df = pd.merge(stats_df, personal_df, on=['player_id', 'name'], how='outer')\n",
    "df = df.fillna('N/A')\n",
    "\n",
    "# Combine all info into a single text field for semantic search\n",
    "def row_to_text(row):\n",
    "    return f\"Name: {row['name']}. Role: {row['role']}. Team: {row['team']}. Batting Avg: {row['batting_average']}. Bowling Avg: {row['bowling_average']}. DOB: {row['date_of_birth']}. Country: {row['country']}. City: {row['city']}.\"\n",
    "\n",
    "df['text'] = df.apply(row_to_text, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6ac37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Vectorize Data with GPT-4.0 Embeddings\n",
    "\n",
    "We use the OpenAI GPT-4.0 embedding model to convert each player's information into a semantic vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23fa1808",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import 'transformers.modeling_utils'. Run 'pip install farm-haystack[inference]'. Original error: cannot import name 'SequenceSummary' from 'transformers.modeling_utils' (/Users/mathewthomas/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/haystack/nodes/retriever/dense.py:38\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, AutoTokenizer\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhaystack\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlanguage_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_language_model, DPREncoder\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhaystack\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbiadaptive_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BiAdaptiveModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/haystack/modeling/model/language_model.py:34\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, AutoModel, PretrainedConfig, PreTrainedModel\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SequenceSummary\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhaystack\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelingError\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'SequenceSummary' from 'transformers.modeling_utils' (/Users/mathewthomas/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Set up Ollama local embedding model (e.g., nomic-embed-text, llama2, etc.)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# You must have Ollama running locally and the model pulled (e.g., `ollama pull nomic-embed-text`)\u001b[39;00m\n\u001b[32m      3\u001b[39m ollama_embedding_model = \u001b[33m\"\u001b[39m\u001b[33mnomic-embed-text\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Or another supported embedding model in Ollama\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m retriever = \u001b[43mEmbeddingRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mollama_embedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence_transformers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use sentence-transformers for local models\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_gpu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Prepare Haystack documents\u001b[39;00m\n\u001b[32m     11\u001b[39m documents = [\n\u001b[32m     12\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mmeta\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mplayer_id\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mplayer_id\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m]}} \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows()\n\u001b[32m     13\u001b[39m  ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/haystack/nodes/base.py:46\u001b[39m, in \u001b[36mexportable_to_yaml.<locals>.wrapper_exportable_to_yaml\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mself\u001b[39m._component_config[\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m][k] = v\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Call the actual __init__ function with all the arguments\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43minit_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/haystack/nodes/retriever/dense.py:1545\u001b[39m, in \u001b[36mEmbeddingRetriever.__init__\u001b[39m\u001b[34m(self, embedding_model, document_store, model_version, use_gpu, batch_size, max_seq_len, model_format, pooling_strategy, query_prompt, passage_prompt, emb_extraction_layer, top_k, progress_bar, devices, use_auth_token, scale_score, embed_meta_fields, api_key, azure_api_version, azure_base_url, azure_deployment_name, api_base, openai_organization, aws_config)\u001b[39m\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1451\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1452\u001b[39m     embedding_model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1475\u001b[39m     aws_config: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1476\u001b[39m ):\n\u001b[32m   1477\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1478\u001b[39m \u001b[33;03m    :param document_store: An instance of DocumentStore from which to retrieve documents.\u001b[39;00m\n\u001b[32m   1479\u001b[39m \u001b[33;03m    :param embedding_model: Local path or name of model in Hugging Face's model hub such\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1543\u001b[39m \u001b[33;03m    :param aws_config: The aws_config contains {aws_access_key, aws_secret_key, aws_region, profile_name} to use with the boto3 Session for an AWS Bedrock retriever. Defaults to 'None'.\u001b[39;00m\n\u001b[32m   1544\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1545\u001b[39m     \u001b[43mtorch_and_transformers_import\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1547\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m embed_meta_fields \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1548\u001b[39m         embed_meta_fields = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/lazy_imports/try_import.py:107\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    106\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Failed to import 'transformers.modeling_utils'. Run 'pip install farm-haystack[inference]'. Original error: cannot import name 'SequenceSummary' from 'transformers.modeling_utils' (/Users/mathewthomas/Documents/hobby_projects/AI_ML_Work/567COG_DS_AI_ML/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py)"
     ]
    }
   ],
   "source": [
    "# Set up Ollama local embedding model (e.g., nomic-embed-text, llama2, etc.)\n",
    "# You must have Ollama running locally and the model pulled (e.g., `ollama pull nomic-embed-text`)\n",
    "ollama_embedding_model = \"nomic-embed-text\"  # Or another supported embedding model in Ollama\n",
    "retriever = EmbeddingRetriever(\n",
    "    embedding_model=ollama_embedding_model,\n",
    "    model_format=\"sentence_transformers\",  # Use sentence-transformers for local models\n",
    "    use_gpu=True\n",
    " )\n",
    "\n",
    "# Prepare Haystack documents\n",
    "documents = [\n",
    "    {\"content\": row['text'], \"meta\": {\"player_id\": row['player_id'], \"name\": row['name']}} for _, row in df.iterrows()\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac2a03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Set Up Backend Database for Vector Storage\n",
    "\n",
    "We will use FAISS as the vector database, storing the index in the `Databases/faiss` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25aeb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up FAISS document store\n",
    "faiss_dir = '../../../../../../Databases/faiss'\n",
    "os.makedirs(faiss_dir, exist_ok=True)\n",
    "faiss_index_path = os.path.join(faiss_dir, 'cricket_faiss_index')\n",
    "\n",
    "# If index exists, load; else, create new\n",
    "if os.path.exists(faiss_index_path + '.faiss'):\n",
    "    document_store = FAISSDocumentStore.load(faiss_index_path)\n",
    "else:\n",
    "    document_store = FAISSDocumentStore(embedding_dim=1536, faiss_index_factory_str=\"Flat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a78248",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Store Vectorized Data in Database\n",
    "\n",
    "We will write the vectorized documents to the FAISS document store for efficient semantic retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write documents and embeddings to FAISS\n",
    "document_store.write_documents(documents)\n",
    "document_store.update_embeddings(retriever)\n",
    "document_store.save(faiss_index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a8cdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Build Haystack Pipeline for Semantic Search\n",
    "\n",
    "We will create a ConversationalRetrievalPipeline using the FAISS retriever and a GPT-4.0 prompt node for answering questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Haystack pipeline with Ollama LLM (e.g., llama2, mistral, etc.)\n",
    "ollama_llm = \"llama2\"  # Or another supported LLM in Ollama\n",
    "ollama_api_url = \"http://localhost:11434/v1/completions\"  # Default Ollama API endpoint\n",
    "\n",
    "prompt_node = PromptNode(\n",
    "    model_name_or_path=ollama_llm,\n",
    "    api_url=ollama_api_url,\n",
    "    max_length=512,\n",
    "    default_prompt_template=PromptTemplate(\n",
    "        name=\"cricket-qa\",\n",
    "        prompt=\"Answer the user's question about cricket players using the provided context. Context: {join(documents)}. Question: {query}\"\n",
    "    ),\n",
    "    model_kwargs={\"temperature\": 0.2},\n",
    "    stop_words=[\"\\n\"]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_node(component=retriever, name=\"Retriever\", inputs=[\"Query\"])\n",
    "pipeline.add_node(component=prompt_node, name=\"PromptNode\", inputs=[\"Retriever\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c7740a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Create Conversational UI for Data Interaction\n",
    "\n",
    "We will use Streamlit to create a simple chat interface for asking questions about the cricket data. Conversations will be persisted efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32edfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit UI for conversation\n",
    "st.title(\"Cricket Data Conversational QA\")\n",
    "\n",
    "# SQLite for conversation persistence\n",
    "conv_db_path = '../../../../../../Databases/cricket_conversations.db'\n",
    "conn = sqlite3.connect(conv_db_path)\n",
    "c = conn.cursor()\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS conversations (id INTEGER PRIMARY KEY, user_input TEXT, response TEXT)''')\n",
    "conn.commit()\n",
    "\n",
    "# Chat interface\n",
    "if 'history' not in st.session_state:\n",
    "    st.session_state['history'] = []\n",
    "\n",
    "user_input = st.text_input(\"Ask a question about cricket players:\")\n",
    "if st.button(\"Ask\") and user_input:\n",
    "    result = pipeline.run({\"Query\": user_input, \"params\": {\"Retriever\": {\"top_k\": 5}}})\n",
    "    answers = result.get('PromptNode', [])\n",
    "    answer = answers[0].answer if answers else \"No answer found.\"\n",
    "    st.session_state['history'].append((user_input, answer))\n",
    "    # Persist to DB\n",
    "    c.execute(\"INSERT INTO conversations (user_input, response) VALUES (?, ?)\", (user_input, answer))\n",
    "    conn.commit()\n",
    "\n",
    "# Display conversation history\n",
    "st.subheader(\"Conversation History\")\n",
    "for q, a in st.session_state['history']:\n",
    "    st.markdown(f\"**You:** {q}\")\n",
    "    st.markdown(f\"**Bot:** {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152ecdd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Persist Conversations Efficiently\n",
    "\n",
    "Conversations are stored in a lightweight SQLite database (`Databases/cricket_conversations.db`). Each user question and bot response is saved for later retrieval and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90aa7b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Notes: Database Installation, Deletion, and Conversation Storage Details\n",
    "\n",
    "- **Database Installation:**\n",
    "    - FAISS is installed via pip and stores its index in the `Databases/faiss` folder.\n",
    "    - SQLite is used for conversation persistence and stores data in `Databases/cricket_conversations.db`.\n",
    "- **Database Deletion:**\n",
    "    - To delete the FAISS index: `rm -rf ../../../../../../Databases/faiss`\n",
    "    - To delete conversation history: `rm ../../../../../../Databases/cricket_conversations.db`\n",
    "- **Conversation Storage:**\n",
    "    - Each user question and bot response is stored as a row in the SQLite database for efficient retrieval and analysis.\n",
    "\n",
    "---\n",
    "\n",
    "You now have a full Haystack pipeline for semantic search and conversational QA over cricket data, with persistent and manageable storage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
